{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sort import Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–∑—è—Ç yolo. –ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å–¥–µ–ª–∞–Ω–Ω–æ–º –≤ CVAT. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –æ–±—É—á–µ–Ω–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of models on test data\n",
    "\n",
    "| Model | Box P | Box R | mAP50 | mAP50-95 | inference ms|\n",
    "|-------|-------|-------|-------|----------|-------------|\n",
    "| yolo11n | 0.7 | 0.502 | 0.611 | 0.284 | 1.7 |\n",
    "| yolo11s | 0.747 | 0.577 | 0.683 | 0.331 | 2.9 |\n",
    "| yolo11m | 0.743 | 0.621 | 0.687 | 0.33 | 6.8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of pre-trained models on test data\n",
    "\n",
    "| Model | Box P | Box R | mAP50 | mAP50-95 | inference ms|\n",
    "|-------|-------|-------|-------|----------|-------------|\n",
    "| yolo11n | 0.683 | 0.457 | 0.532 | 0.221 | 2.0 |\n",
    "| yolo11s | 0.746 | 0.418 | 0.534 | 0.203 | 4.2 |\n",
    "| yolo11m | 0.71 | 0.5 | 0.595 | 0.244 | 6.9 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Å —Ç–æ–ª–ø–æ–π –Ω–µ –ø–æ–≤—ã—Å–∏–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏ —Å–¥–µ–ª–∞—Ç—å –µ–≥–æ –±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ù–æ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –±–æ–ª–µ–µ –Ω–∞–∑–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–ø—Ä–∞–≤–∏–ª–∏—Å—å –ª—É—á—à–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ–æ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yolo11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11n.pt')\n",
    "results = model.train(data='my_dataset_yolo/data.yaml', epochs=100, imgsz=640, device=DEVICE)\n",
    "\n",
    "# YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
    "#                  Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.18it/s]\n",
    "#                    all         17        217      0.834      0.834      0.874      0.434\n",
    "# Speed: 1.4ms preprocess, 16.0ms inference, 0.0ms loss, 1.0ms postprocess per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81 üöÄ Python-3.9.21 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7944MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/daniluck505/Documents/Projects/tracking crowd/my_dataset_yolo/labels/test.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12        302      0.683      0.457      0.532      0.221\n",
      "Speed: 0.1ms preprocess, 2.0ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/home/daniluck505/Documents/Projects/tracking crowd/runs/detect/train/weights/best.pt')\n",
    "model.val(data='my_dataset_yolo/test_data.yaml');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81 üöÄ Python-3.9.21 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7944MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/daniluck505/Documents/Projects/tracking crowd/my_dataset_yolo/labels/test.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12        302        0.7      0.502      0.611      0.284\n",
      "                person         12        302        0.7      0.502      0.611      0.284\n",
      "Speed: 0.1ms preprocess, 1.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolo11n.pt')\n",
    "model.val(data='my_dataset_yolo/test_data.yaml');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yolo11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11s.pt')\n",
    "results = model.train(data='my_dataset_yolo/data.yaml', epochs=100, imgsz=640, device=DEVICE)\n",
    "\n",
    "# YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n",
    "#                  Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.36it/s]\n",
    "#                    all         17        217      0.897      0.797      0.883      0.453\n",
    "# Speed: 1.1ms preprocess, 17.5ms inference, 0.0ms loss, 0.3ms postprocess per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81 üöÄ Python-3.9.21 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7944MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/daniluck505/Documents/Projects/tracking crowd/my_dataset_yolo/labels/test.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12        302      0.746      0.418      0.534      0.203\n",
      "Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/home/daniluck505/Documents/Projects/tracking crowd/runs/detect/train2/weights/best.pt')\n",
    "model.val(data='my_dataset_yolo/test_data.yaml');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81 üöÄ Python-3.9.21 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7944MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/daniluck505/Documents/Projects/tracking crowd/my_dataset_yolo/labels/test.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12        302      0.747      0.577      0.683      0.331\n",
      "                person         12        302      0.747      0.577      0.683      0.331\n",
      "Speed: 0.1ms preprocess, 2.9ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolo11s.pt')\n",
    "model.val(data='my_dataset_yolo/test_data.yaml');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yolo11m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11m.pt')\n",
    "results = model.train(data='my_dataset_yolo/data.yaml', epochs=100, imgsz=640, device=DEVICE, batch=10)\n",
    "\n",
    "# YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n",
    "#                  Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  3.08it/s]\n",
    "#                    all         17        217      0.901      0.797       0.89      0.452\n",
    "# Speed: 1.2ms preprocess, 20.2ms inference, 0.0ms loss, 0.4ms postprocess per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81 üöÄ Python-3.9.21 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7944MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/daniluck505/Documents/Projects/tracking crowd/my_dataset_yolo/labels/test.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12        302       0.71        0.5      0.595      0.244\n",
      "Speed: 0.1ms preprocess, 6.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/home/daniluck505/Documents/Projects/tracking crowd/runs/detect/train3/weights/best.pt')\n",
    "model.val(data='my_dataset_yolo/test_data.yaml');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.81 üöÄ Python-3.9.21 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7944MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,091,712 parameters, 0 gradients, 68.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/daniluck505/Documents/Projects/tracking crowd/my_dataset_yolo/labels/test.cache... 12 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         12        302      0.743      0.621      0.687       0.33\n",
      "                person         12        302      0.743      0.621      0.687       0.33\n",
      "Speed: 0.1ms preprocess, 6.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolo11m.pt')\n",
    "model.val(data='my_dataset_yolo/test_data.yaml');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoBuilder:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π.\n",
    "    –ê—Ç—Ä–∏–±—É—Ç—ã:\n",
    "        transforms (list): –°–ø–∏—Å–æ–∫ —Ñ—É–Ω–∫—Ü–∏–π-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π, –ø—Ä–∏–º–µ–Ω—è–µ–º—ã—Ö –∫ –∫–∞–∂–¥–æ–º—É –∫–∞–¥—Ä—É.\n",
    "        fps_counter (FPS_Counter): –°—á–µ—Ç—á–∏–∫ FPS –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –∫–∞–¥—Ä–µ.\n",
    "        sort_tracker (Sort): –¢—Ä–µ–∫–µ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ SORT.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.transforms = []\n",
    "        self.fps_counter = None\n",
    "        self.sort_tracker = None\n",
    "    \n",
    "    def resize(self, scale=0.5):\n",
    "        \"\"\"\n",
    "        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            scale (float): –ú–∞—Å—à—Ç–∞–± –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.5.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            self: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ –≤—ã–∑–æ–≤–æ–≤.\n",
    "        \"\"\"\n",
    "        self.transforms.append(lambda frame: cv2.resize(frame, (-1, -1), fx=scale, fy=scale))\n",
    "        return self\n",
    "\n",
    "    def fps(self, calc_time_perion_N_frames=10):\n",
    "        \"\"\"\n",
    "        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è FPS –Ω–∞ –∫–∞–¥—Ä–µ.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            calc_time_perion_N_frames (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ FPS. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 10.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            self: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ –≤—ã–∑–æ–≤–æ–≤.\n",
    "        \"\"\"\n",
    "        if not self.fps_counter:\n",
    "            self.fps_counter = FPS_Counter(calc_time_perion_N_frames)\n",
    "        \n",
    "        def display_fps(frame):\n",
    "            fps_real = self.fps_counter.calc_FPS()\n",
    "            text = f\"FPS: {fps_real:.1f}\"\n",
    "\n",
    "            fontFace = 1\n",
    "            fontScale = 1.3\n",
    "            thickness = 1\n",
    "            \n",
    "            (label_width, label_height), _ = cv2.getTextSize(\n",
    "                text,\n",
    "                fontFace=fontFace,\n",
    "                fontScale=fontScale,\n",
    "                thickness=thickness,\n",
    "            )\n",
    "            frame = cv2.rectangle(frame, (0, 0), (10 + label_width, 15 + label_height), (0, 0, 0), -1)\n",
    "            frame = cv2.putText(\n",
    "                img=frame,\n",
    "                text=text,\n",
    "                org=(5, 20),\n",
    "                fontFace=fontFace,\n",
    "                fontScale=fontScale,\n",
    "                thickness=thickness,\n",
    "                color=(255, 255, 255),\n",
    "            )\n",
    "            return frame\n",
    "        \n",
    "        self.transforms.append(display_fps)\n",
    "        return self\n",
    "\n",
    "    def detection(self, model, imgsz=640, conf=0.7, iou=0.7, show_text=True, only_person=True):\n",
    "        \"\"\"\n",
    "        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä–µ.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "            imgsz (int): –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 640.\n",
    "            conf (float): –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.7.\n",
    "            iou (float): –ü–æ—Ä–æ–≥ IoU –¥–ª—è NMS. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.7.\n",
    "            show_text (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∫–∞–¥—Ä–µ. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.\n",
    "            only_person (bool): –§–ª–∞–≥ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ª—é–¥–µ–π. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            self: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ –≤—ã–∑–æ–≤–æ–≤.\n",
    "        \"\"\"\n",
    "        self.transforms.append(lambda frame: self.__visualize_detection(\n",
    "                                            frame,\n",
    "                                            model,\n",
    "                                            imgsz,\n",
    "                                            conf,\n",
    "                                            iou,\n",
    "                                            show_text=show_text,\n",
    "                                            only_person=only_person\n",
    "                                        ))\n",
    "        return self\n",
    "\n",
    "    def __visualize_detection(self, img, model, imgsz, conf, iou, show_text, only_person):\n",
    "        \"\"\"\n",
    "        –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∫–∞–¥—Ä–µ.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            img: –í—Ö–æ–¥–Ω–æ–π –∫–∞–¥—Ä.\n",
    "            model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "            imgsz (int): –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏.\n",
    "            conf (float): –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è.\n",
    "            iou (float): –ü–æ—Ä–æ–≥ IoU –¥–ª—è NMS.\n",
    "            show_text (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∫–∞–¥—Ä–µ.\n",
    "            only_person (bool): –§–ª–∞–≥ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ª—é–¥–µ–π.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            img: –ö–∞–¥—Ä —Å –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏.\n",
    "        \"\"\"\n",
    "        predictions = model.predict(img, imgsz=imgsz, conf=conf, iou=iou, verbose=False)\n",
    "\n",
    "        labeled_image = img.copy()\n",
    "\n",
    "        if self.sort_tracker:\n",
    "            return self.__plot_bboxes_with_sort(labeled_image, predictions, show_text, only_person)\n",
    "        else:\n",
    "            class_names = model.names\n",
    "            return self.__plot_bboxes_with_class(labeled_image, predictions, class_names, show_text, only_person)\n",
    "    \n",
    "    def __plot_bboxes_with_class(self, frame, predictions, class_names, show_class, only_person):\n",
    "        \"\"\"\n",
    "        –†–∏—Å—É–µ—Ç bounding box'—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ –Ω–∞ –∫–∞–¥—Ä–µ.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            frame: –í—Ö–æ–¥–Ω–æ–π –∫–∞–¥—Ä.\n",
    "            predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "            class_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤.\n",
    "            show_class (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞ –Ω–∞ bounding box'–µ.\n",
    "            only_person (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ª—é–¥–µ–π.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            frame: –ö–∞–¥—Ä —Å bounding box'–∞–º–∏.\n",
    "        \"\"\"\n",
    "        for pred in predictions:\n",
    "            boxes = pred.boxes.xyxy.cpu().int().tolist()\n",
    "            classes = pred.boxes.cls.cpu().int().tolist()\n",
    "            confidences = pred.boxes.conf.cpu().numpy()\n",
    "\n",
    "            for i, (box, class_index) in enumerate(zip(boxes, classes)):\n",
    "                if only_person and int(class_index) != 0:\n",
    "                    continue\n",
    "                class_name = class_names[int(class_index)]\n",
    "                random.seed(int(classes[i])+10)\n",
    "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 4)\n",
    "\n",
    "                if show_class:\n",
    "                    label = f'{class_name} {confidences[i]:.2f}'\n",
    "                    (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "                    cv2.rectangle(frame, (x_min, y_min - text_height - 10), (x_min + text_width, y_min), color, -1)\n",
    "                    cv2.putText(frame, label, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        return frame\n",
    "\n",
    "    def __plot_bboxes_with_sort(self, frame, predictions, show_ids, only_person):\n",
    "        \"\"\"\n",
    "        –†–∏—Å—É–µ—Ç bounding box'—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—Ä–µ–∫–µ—Ä–∞ SORT.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            frame: –í—Ö–æ–¥–Ω–æ–π –∫–∞–¥—Ä.\n",
    "            predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "            show_ids (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è ID –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "            only_person (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ª—é–¥–µ–π.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            frame: –ö–∞–¥—Ä —Å bounding box'–∞–º–∏ –∏ ID –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "        \"\"\"\n",
    "        detections_list = []\n",
    "        for pred in predictions:\n",
    "            boxes = pred.boxes.xyxy.cpu().int().tolist()\n",
    "            classes = pred.boxes.cls.cpu().int().tolist()\n",
    "            confidences = pred.boxes.conf.cpu().numpy()\n",
    "\n",
    "            for i, (box, class_index) in enumerate(zip(boxes, classes)):\n",
    "                if only_person and int(class_index) != 0:\n",
    "                    continue\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                detections_list.append([x_min, y_min, x_max, y_max, confidences[i]])\n",
    "\n",
    "        if len(detections_list) == 0:\n",
    "                detections_list = np.empty((0, 5))\n",
    "\n",
    "        res = self.sort_tracker.update(np.array(detections_list))\n",
    "        for track in res:\n",
    "            x_min, y_min, x_max, y_max, track_id = track\n",
    "            random.seed(15)\n",
    "            color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), color, 4)\n",
    "            if show_ids:\n",
    "                cv2.putText(frame, f\"ID: {int(track_id)}\", (int(x_min), int(y_min - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def sort(self, max_age=100, min_hits=8, iou_threshold=0.50, show_ids=True):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–∫–µ—Ä SORT.\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            max_age (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–µ–∫–∞ –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 100.\n",
    "            min_hits (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø–∞–¥–∞–Ω–∏–π –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–∫–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 8.\n",
    "            iou_threshold (float): –ü–æ—Ä–æ–≥ IoU –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–∫–æ–≤. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.50.\n",
    "            show_ids (bool): –§–ª–∞–≥ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è ID –æ–±—ä–µ–∫—Ç–æ–≤. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é True.\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            self: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ü–µ–ø–æ—á–∫–∏ –≤—ã–∑–æ–≤–æ–≤.\n",
    "        \"\"\"\n",
    "        self.sort_tracker = Sort(max_age=max_age, \n",
    "                                 min_hits=min_hits, \n",
    "                                 iou_threshold=iou_threshold) \n",
    "        self.show_ids = show_ids\n",
    "        return self\n",
    "\n",
    "    def stream_camera(self, camera=0, size=(720, 480)):\n",
    "        \"\"\"\n",
    "        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤–æ–µ –≤–∏–¥–µ–æ —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            camera (int): –ò–Ω–¥–µ–∫—Å –∫–∞–º–µ—Ä—ã. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.\n",
    "            size (tuple): –†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞ (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é (720, 480).\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(camera)\n",
    "        assert cap.isOpened(), '–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É'\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, size[0])\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, size[1])\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            assert ret, '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã'\n",
    "            \n",
    "            transformed_frame = frame.copy()\n",
    "            for transform in self.transforms:\n",
    "                transformed_frame = transform(transformed_frame)            \n",
    "            \n",
    "            cv2.imshow('Webcam', transformed_frame)\n",
    "\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def stream_video(self, video_path, video_out_path=None, out_name='out.mp4'):\n",
    "        \"\"\"\n",
    "        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤–æ–µ –≤–∏–¥–µ–æ –∏–∑ —Ñ–∞–π–ª–∞ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π.\n",
    "\n",
    "        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "            video_path (str): –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—É.\n",
    "            video_out_path (str): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.\n",
    "            out_name (str): –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 'out.mp4'.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if video_out_path:\n",
    "            for transform in self.transforms:\n",
    "                frame = transform(frame)\n",
    "            video_out_path = os.path.join(video_out_path, out_name)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')            \n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_width = frame.shape[1]\n",
    "            frame_height = frame.shape[0]\n",
    "            frame_size = (int(frame_width), int(frame_height))\n",
    "\n",
    "            cap_out = cv2.VideoWriter(video_out_path, \n",
    "                                    fourcc, \n",
    "                                    fps,\n",
    "                                    frame_size)\n",
    "        \n",
    "        while ret:\n",
    "            transformed_frame = frame.copy()\n",
    "            for transform in self.transforms:\n",
    "                transformed_frame = transform(transformed_frame)\n",
    "\n",
    "            cv2.imshow(video_path, transformed_frame)\n",
    "            if video_out_path:\n",
    "                cap_out.write(transformed_frame)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            k = cv2.waitKey(1)\n",
    "            if k == ord('q'):\n",
    "                break\n",
    "        \n",
    "        \n",
    "        cap.release()\n",
    "        if video_out_path:\n",
    "            cap_out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "class FPS_Counter:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ FPS (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É).\n",
    "\n",
    "    –ê—Ç—Ä–∏–±—É—Ç—ã:\n",
    "        time_buffer (list): –ë—É—Ñ–µ—Ä –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫.\n",
    "        calc_time_perion_N_frames (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ FPS.\n",
    "    \"\"\"\n",
    "    def __init__(self, calc_time_perion_N_frames):\n",
    "        self.time_buffer = []\n",
    "        self.calc_time_perion_N_frames = calc_time_perion_N_frames\n",
    "\n",
    "    def calc_FPS(self):\n",
    "        time_buffer_is_full = len(self.time_buffer) == self.calc_time_perion_N_frames\n",
    "        t = time.time()\n",
    "        self.time_buffer.append(t)\n",
    "\n",
    "        if time_buffer_is_full:\n",
    "            self.time_buffer.pop(0)\n",
    "            fps = len(self.time_buffer) / (self.time_buffer[-1] - self.time_buffer[0])\n",
    "            return np.round(fps, 2)\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = VideoBuilder()\n",
    "stream.fps().stream_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = VideoBuilder()\n",
    "imgsz, conf, iou, = 640, 0.7, 0.5\n",
    "model = YOLO('yolo11n.pt')\n",
    "stream.resize(1.5).detection(model, imgsz, conf, iou, True).fps().stream_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = VideoBuilder()\n",
    "imgsz, conf, iou, = 640, 0.7, 0.5\n",
    "model = YOLO('yolo11n.pt')\n",
    "max_age, min_hits, iou_threshold, show_ids = 100, 10, 0.5, True\n",
    "stream.resize(1.5).detection(model, imgsz, conf, iou, True).fps().sort(max_age, min_hits, iou_threshold, show_ids).stream_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'videos/test.mp4'\n",
    "save_path = 'videos/out'\n",
    "\n",
    "imgsz, conf, iou = 640, 0.6, 0.5\n",
    "max_age, min_hits, iou_threshold, show_ids = 100, 10, 0.5, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = VideoBuilder()\n",
    "# model = YOLO('yolo11m.pt')\n",
    "model = YOLO('/home/daniluck505/Documents/Projects/tracking crowd/runs/detect/train3/weights/best.pt')\n",
    "stream.detection(model, imgsz, conf, iou, True).stream_video(video_path, save_path, 'out_preyolo11m.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = VideoBuilder()\n",
    "# model = YOLO('yolo11m.pt')\n",
    "model = YOLO('/home/daniluck505/Documents/Projects/tracking crowd/runs/detect/train3/weights/best.pt')\n",
    "stream.detection(model, imgsz, conf, iou, True).sort(max_age, min_hits, iou_threshold, show_ids).stream_video(video_path, save_path, 'out_preyolo11m_sort.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ SORT –ø–æ–∑–≤–æ–ª–∏–ª–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–ª–∞–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–æ–≤–∞–Ω–∏–∏."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
